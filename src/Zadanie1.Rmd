---
title: "Semestral Project: Analysis of Obesity Levels (Regression & Classification)"
author: "David Wagner"
date: "`r format(Sys.Date(), '%d.%m.%Y')`"
output: 
  html_document:
    toc: true
    toc_float: true
---

# Introduction

In this project, we delve into a dataset that captures the obesity levels among individuals from Mexico, Peru, and Colombia. The dataset can be found at [Kaggle](https://www.kaggle.com/datasets/aravindpcoder/obesity-or-cvd-risk-classifyregressorcluster?resource=download).
This rich dataset encompasses 2,111 records and 17 attributes, highlighting diverse eating habits, physical conditions, and demographic details of participants aged between 14 and 61. Our analysis aims to uncover patterns and predictions related to obesity levels through two main hypotheses:

* **Regression Hypothesis:** We hypothesize that an individual's body weight can be significantly predicted by a combination of their height, frequency of consuming high-calorie food, frequency of vegetable consumption, number of main meals per day, and physical activity frequency. These factors collectively represent an individual's lifestyle and physical condition, offering insights into how these variables contribute to body weight variations.

* **Classification Hypothesis:** We aim to classify individuals into two categories: those at risk of obesity and those not, by examining the influence of high-calorie food consumption frequency, the number of main meals, consumption of food between meals, alcohol consumption, age, and gender. This model seeks to understand how these dietary habits, demographic information, and personal behaviors correlate with the risk of obesity.

These hypotheses guide our exploration and analysis, contributing to a deeper understanding of the factors influencing obesity.


# Data Loading and Preprocessing

To initiate our analysis, the first step involves loading the dataset into our R environment. This dataset, stored as "ObesityDataSet.csv", encapsulates vital information on obesity levels among individuals from specific Latin American countries. Our analysis begins by setting the working directory to where the dataset is located and subsequently loading the dataset into R for inspection and preprocessing.

```{r load-data, message=FALSE, warning=FALSE}
library(readr)
library(tidyverse)
library(lobstr)
library(dplyr)
library(ggplot2)
library(magrittr)
library(data.table)
library(caret)
library(MASS)
# library(cutpointr)
library(purrr)
library(GGally)
library(ggplot2)
library(gplots)
library(pROC)


# Setting the working directory to where the dataset is located
# Uncomment the appropriate line below to match your directory structure

# setwd("C:/Users/dswag/Desktop/Zadania/OZNAL/Data")
setwd("C:/work/oznal/OZNAL_zadania/Data")

# Listing all files in the current working directory to verify the presence of our dataset
list.files()

# Reading the dataset into R
raw_data <- read_csv("ObesityDataSet.csv", col_names = TRUE, num_threads = 4)

# Displaying the first few rows of the dataset to ensure it's loaded correctly
head(raw_data)


# Renaming columns in the dataset to make them more descriptive
names(raw_data)[names(raw_data) == "FAVC"] <- "FreqConsHighCalFood"
names(raw_data)[names(raw_data) == "FCVC"] <- "FreqConsVegs"
names(raw_data)[names(raw_data) == "NCP"] <- "NumMainMeals"
names(raw_data)[names(raw_data) == "CAEC"] <- "ConsFoodBetwMeals"
names(raw_data)[names(raw_data) == "CH2O"] <- "ConsWaterDaily"
names(raw_data)[names(raw_data) == "CALC"] <- "ConsAlc"
names(raw_data)[names(raw_data) == "SCC"] <- "CalsConsMon"
names(raw_data)[names(raw_data) == "FAF"] <- "PhysActFreq"
names(raw_data)[names(raw_data) == "TUE"] <- "TimeTechDev"
names(raw_data)[names(raw_data) == "MTRANS"] <- "Trans"

# Identifying missing values in dataset
sum(is.na(raw_data))

# Identifying missing values per column
colSums(is.na(raw_data))

# Function to identify outliers within one column (instead of 0.25 and 0.75 quantiles we used 0.15 and 0.85 to maintain a reasonable dataset size)
remove_outliers <- function(x) {
  # Ensure the column is numeric
  if(is.numeric(x)) {
    Q1 <- quantile(x, 0.15, na.rm = TRUE)
    Q3 <- quantile(x, 0.85, na.rm = TRUE)
    IQR <- Q3 - Q1
    lower_bound <- Q1 - 1.5 * IQR
    upper_bound <- Q3 + 1.5 * IQR
    x[x < lower_bound | x > upper_bound] <- NA  # Assign NA to outliers
  }
  x
}

# Apply the function to each column
cleaned_data <- raw_data %>% mutate(across(where(is.numeric), remove_outliers))

# If removing outliers introduced NAs, remove those rows
cleaned_data %<>% drop_na()
raw_data
cleaned_data

# Check for missing values again
sum(is.na(cleaned_data))

# Check data types of each column
str(cleaned_data)

# Convert all character columns to factors to represent categorical data
cleaned_data <- cleaned_data %>%
  mutate(across(where(is.character), as.factor))

# Check factorization of each column
str(cleaned_data)

# Removing duplicate rows from the dataset, keeping only unique rows
cleaned_data <- cleaned_data %>% distinct()

cleaned_data

```

# Exploratory Data Analysis

Our exploratory data analysis revealed significant relationships within the dataset, particularly between height and weight, and age and weight, indicating that taller and older individuals tend to have a higher body weight. However, correlations between lifestyle factors such as physical activity frequency and technology use with age suggest potential areas for targeted interventions. The use of visualizations, including scatter plots and heatmaps, allowed for a comprehensive understanding of the relationships and distribution of variables within our dataset.

```{r eda-data, message=FALSE, warning=FALSE}
# Select only the numeric columns for the pairs plot
numerical_data <- cleaned_data[sapply(cleaned_data, is.numeric) ]

# Use ggpairs to create the pairs plot, displaying both scatter plots and correlations
pairs(numerical_data)

# Calculate correlation matrix for numerical columns
cor_matrix <- cor(numerical_data, use = "complete.obs")

cor_matrix
```

**Positive Correlations:**

* Height and Weight (0.46)
* Age and Weight (0.22)
* Weight and ConsWaterDaily (0.22)

**Negative Correlations:**

* Age and PhysActFreq (-0.16)
* Age and TimeTechDev (-0.30)

Most other correlations are weak, indicating little linear relationship.

```{r eda-data1, message=FALSE, warning=FALSE}
# Plotting the heatmap using base R's heatmap() function
heatmap.2(x = cor_matrix,
          scale = "none", 
          Colv = NA, 
          Rowv = NA, 
          dendrogram = "none",
          trace = "none", 
          col = colorRampPalette(c("blue", "white", "red"))(n = 299), 
          margin = c(5, 5), 
          main = "Correlation Matrix Heatmap",
          key = TRUE, # This ensures the color key (legend) is displayed
          keysize = 1.5)

```



# Regression Analysis: Predicting Weight

## Hypothesis

**Null Hypothesis (H0):** There is no significant relationship between an individual's body weight and the independent variables of height, frequency of consuming high-calorie food, frequency of vegetable consumption, number of main meals per day, and physical activity frequency. In other words, these lifestyle and physical condition factors do not predict an individual's body weight.

**Alternative Hypothesis (H1):** There is a significant relationship between an individual's body weight and the independent variables of height, frequency of consuming high-calorie food, frequency of vegetable consumption, number of main meals per day, and physical activity frequency. These factors collectively serve as significant predictors of an individual's body weight.

## Data Preparation
The data was already cleaned and prepared for model building, since the dataset didn't contain any missing values. The categorical columns were factorized and the outliers removed. Let's create a regression formula based on our hypothesis.
```{r lm-data, message=FALSE, warning=FALSE}
# Create a reggresion formula
regression_formula <- Weight ~ Height + FreqConsHighCalFood + FreqConsVegs + NumMainMeals + PhysActFreq
```


## Model Building

The regression model was constructed to predict body weight based on height, frequency of consuming high-calorie food, frequency of vegetable consumption, and physical activity frequency. The model building process involved cleaning and preprocessing the data, including outlier removal and factorizing categorical variables, followed by the application of a linear regression formula. Despite the exclusion of the *NumMainMeals* variable due to its insignificance, the model leverages the remaining variables to predict weight.
```{r lm-data1, message=FALSE, warning=FALSE}
# Fit the linear regression model
model <- lm(regression_formula, data = cleaned_data)

# Summarize the model to view coefficients and other statistics
summary(model)

# If you want to check the diagnostic plots for assumptions
par(mfrow = c(2, 2))
plot(model)
```

### Intercept
Estimated weight when all other predictors are zero. In this case, it's **-183.8161**. However, given the nature of weight and the predictors in this context, interpretation of the intercept might not be meaningful.

### Height
For each unit increase in height, weight is estimated to **increase** by approximately **138.2095** units

### FreqConsHighCalFoodyes
If an individual consumes high-calorie foods frequently (represented as "yes"), their weight is estimated to **increase** by about **14.0116** units compared to those who don't consume high-calorie foods frequently

### FreqConsVegs
For each unit increase in the frequency of consuming vegetables, weight is estimated to **increase** by about **12.0400** units

### NumMainMeals
It seems to have a **insignificant** effect of the number of main meals on weight, as the coefficient estimate is close to zero and the **p-value** is **greater** than the typical significance level of **0.05**. This suggests that the number of main meals doesn't significantly contribute to explaining the variation in weight in this model.

### PhysActFreq
For each unit increase in the frequency of physical activity, weight is estimated to **decrease** by about **-5.7360** units.

### Residuals
These are the differences between the observed and predicted values of the dependent variable (weight). The summary statistics for the residuals shows the spread of these differences:

* Min (-58.431) 
* 1Q (-14.580)
* Median (2.077)
* 3Q (15.853)
* Max (60.099)

### Residual standard error
This is an estimate of the standard deviation of the errors in the model. It indicates the average amount that the observed values deviate from the predicted values

### Multiple R-squared and Adjusted R-squared
These are measures of how well the independent variables explain the variation in the dependent variable. In this model, It suggests that about **33.52%** of the variability in weight is explained by the independent variables

### F-statistic and p-value
This tests the overall significance of the model. In this case, the F-statistic is **208.9** with a very low p-value, suggesting that the model as a whole is significant

### Apply insights to the model
First we split the dataset into train/test sets with ratio 8:2 respectively. Eventually we remove the *NumMainMeals* attribute from the regression formula, since it doesn't significantly contribute to explaining the variation in weight.

```{r lm-data2, message=FALSE, warning=FALSE}
# # Creating a new binary variable 'ObesityBinary' where 1 indicates Obesity (I, II, III) and 0 indicates otherwise
cleaned_data$ObesityBinary <- ifelse(cleaned_data$Nobesity %in% c("Obesity_Type_I", "Obesity_Type_II", "Obesity_Type_III"), 1, 0)

set.seed(123) # for reproducibility

# Calculate the size of the training set (80% of the dataset)
training_size <- floor(0.8 * nrow(cleaned_data))

# Sample indices for the training data
training_indices <- sample(seq_len(nrow(cleaned_data)), size = training_size)

# Split the data
train_data <- cleaned_data[training_indices, ]
test_data <- cleaned_data[-training_indices, ]

# Fit the model on the training data
regression_formula <- Weight ~ Height + FreqConsHighCalFood + FreqConsVegs + PhysActFreq
model_train <- lm(regression_formula, data = train_data)

# Use the model to make predictions on the test data
predictions <- predict(model_train, newdata = test_data)
```

## Model Evaluation

Model performance was evaluated using RMSE and MAE, yielding values of approximately 20.46 and 16.95, respectively. These metrics indicate the model's average prediction error in terms of body weight, suggesting that while the model captures the general trend, there's noticeable deviation from actual values. The evaluation highlights the model's predictive accuracy and areas for improvement.
```{r lm-data3, message=FALSE, warning=FALSE}
# Calculate the Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE)
actuals <- test_data$Weight
mae <- mean(abs(actuals - predictions))
rmse <- sqrt(mean((predictions - actuals)^2))

# Print the RMSE and MAE
print(paste("RMSE on test data:", rmse))
print(paste("MAE on test data:", mae))

# Summarize the model to view coefficients and other statistics
summary(model_train)

# If you want to check the diagnostic plots for assumptions
par(mfrow = c(2, 2))
plot(model_train)

```


### RMSE (Root Mean Squared Error)
* RMSE measures the average magnitude of the errors between predicted and actual values, with higher weights given to larger errors due to squaring
* A lower RMSE value indicates better model performance, as it means the model's predictions are closer to the actual values on average.
* In our case, an RMSE of approximately **20.4636** suggests that, on average, out model's predictions are off by around **20.4636** units of the response variable (in this case, weight).

### MAE (Mean Absolute Error)
* MAE measures the average magnitude of the absolute errors between predicted and actual values.
* Like RMSE, a lower MAE value indicates better model performance, as it means the model's predictions are closer to the actual values on average.
* In our case, an MAE of approximately **16.9483** suggests that, on average, our model's predictions are off by around **16.9483** units of the response variable (in this case, weight).

Lower RMSE and MAE values indicate better performance, suggesting that our model is making more accurate predictions on the test data.

## Conclusion
Based on the model's results, we accept the alternative hypothesis (H1) that there is a significant relationship between an individual's body weight and the independent variables of height, frequency of consuming high-calorie food, frequency of vegetable consumption, and physical activity frequency. These factors collectively serve as significant predictors of an individual's body weight, confirming our regression hypothesis. However, the model evaluation suggests a need for further refinement to improve accuracy, potentially by exploring additional predictors or employing more complex modeling techniques.


# Classification Analysis: NObesity Level Classification (Binary)

## Hypothesis

**Null Hypothesis (H0):** There is no significant relationship between the risk of obesity and the independent variables of frequency of consuming high-calorie food, number of main meals, consumption of food between meals, alcohol consumption, age, and gender. These variables do not significantly contribute to predicting whether an individual is at risk of obesity or not.

**Alternative Hypothesis (H1):** There is a significant relationship between the risk of obesity and the independent variables of frequency of consuming high-calorie food, number of main meals, consumption of food between meals, alcohol consumption, age, and gender. These factors collectively serve as significant predictors in classifying individuals into categories indicating their risk of obesity.

We aim to classify individuals into binary obesity levels (1 if Obesity I/II/III, otherwise 0) by analyzing their eating habits, physical condition, and demographic information.

## Data Preparation

All necessary attributes have already been modified.

## Model Building

```{r glm-data, message=FALSE, warning=FALSE}
# Fit the logistic regression model on the training data
classification_model_train <- glm(ObesityBinary ~ FreqConsHighCalFood + NumMainMeals + ConsFoodBetwMeals + ConsAlc + Age + Gender, data = train_data, family = binomial())

# Summarize the model to view coefficients and other statistics
summary(classification_model_train)
```

### FreqConsHighCalFood (yes)
Individuals who frequently consume high-calorie foods are significantly more likely to be classified as obese. Specifically, the model suggests that consuming high-calorie foods frequently increases the odds of being classified as obese by a factor of **exp(2.39902)**, which is approximately **11.01** times higher compared to those who do not frequently consume high-calorie foods.

### NumMainMeals
The number of main meals has a positive but not statistically significant effect on the odds of obesity, with each additional main meal increasing the odds by a factor of **exp(0.08867)**, approximately **1.09** times. However, due to the p-value of **0.242675**, this effect is not considered statistically significant within the context of this model.

### ConsFoodBetwMeals (Frequently)
Frequent consumption of food between meals is associated with a significant decrease in the odds of obesity. Compared to the baseline category, frequent eaters have **exp(-1.57740)**, or about **0.21** times the odds of being classified as obese, indicating a protective effect against obesity.

### ConsFoodBetwMeals (Sometimes)
Eating between meals sometimes, as opposed to never, increases the odds of being classified as obese by a factor of **exp(1.51263)**, or approximately **4.54** times.

### ConsAlc (Frequently, Sometimes, No)
The consumption of alcohol, whether frequently, sometimes, or not at all, shows large coefficients due to the presence of wide standard errors, indicating instability in these estimates likely due to multicollinearity or sparse data within these categories. As such, the interpretation of alcohol consumption's effect on obesity odds from this model should be approached with caution.

### Age
Each additional year of age increases the odds of being classified as obese by a factor of **exp(0.08497)**, or approximately **1.09** times. This suggests a slight but significant increase in obesity risk with age.

### Gender (Male)
Being male is associated with slightly lower odds of being classified as obese compared to females. The model suggests that males have **exp(-0.16339)**, or about **0.85** times the odds of being classified as obese compared to females, though this result is not statistically significant **(p-value = 0.152692)**.

```{r glm-data1, message=FALSE, warning=FALSE}
# Predict probabilities on the test data
test_probabilities <- predict(classification_model_train, newdata = test_data, type = "response")

# Calculate ROC curve and AUC on test data
roc_test_result <- roc(test_data$ObesityBinary, test_probabilities)
plot(roc_test_result)
auc_test <- auc(roc_test_result)

# Print AUC
print(paste("AUC on test data:", auc_test))
```

### AUC (Area Under the ROC Curve)
An AUC of **0.7621** indicates that the model performs reasonably well in distinguishing between the classes, with a higher value suggesting better discrimination than random chance.

```{r glm-data2, message=FALSE, warning=FALSE}
# Convert probabilities to binary outcomes based on a threshold
predicted_outcomes <- ifelse(test_probabilities > 0.5, 1, 0)

# Confusion Matrix
table(Predicted = predicted_outcomes, Actual = test_data$ObesityBinary)

# Creating the confusion matrix with caret
conf_matrix <- confusionMatrix(as.factor(predicted_outcomes), as.factor(test_data$ObesityBinary))
```

* True Negatives (TN): 123
* False Positives (FP): 30
* False Negatives (FN): 98
* True Positives (TP): 163

## Model Evaluation

The classification model demonstrated an accuracy of approximately 68.75%, with a precision of 80.39% and a recall of 55.16%. The F1 score of 0.6542553 indicates a reasonable balance between precision and recall, suggesting the model is effective in identifying individuals at risk of obesity, though there is room for improvement in recall. The log loss of 0.5553242 reflects the model's efficiency in predicting the probability of obesity risk with a moderate degree of certainty.

```{r glm-data3, message=FALSE, warning=FALSE}
# Plotting the confusion matrix with gplots
graphics.off()
heatmap.2(as.matrix(conf_matrix$table),
          Rowv = NA, Colv = NA,
          col = colorRampPalette(c("blue", "white", "red"))(n = 100),
          dendrogram = "none", trace = "none",
          key = TRUE, keysize = 1,
          main = "Confusion Matrix Heatmap",
          xlab = "Actual", ylab = "Predicted")

# Printing the overall statistics from the confusion matrix
print(conf_matrix$overall)

# Calculate log loss
log_loss <- function(predicted_probs, actual_outcomes) {
  N <- length(actual_outcomes)
  loss <- 0
  for (i in 1:N) {
    predicted_prob <- predicted_probs[i]
    actual_outcome <- actual_outcomes[i]
    instance_loss <- ifelse(actual_outcome == 1, -log(predicted_prob), -log(1 - predicted_prob))
    loss <- loss + instance_loss
  }
  loss <- loss / N
  return(loss)
}

# Specific metrics can be accessed directly
accuracy <- conf_matrix$overall['Accuracy']
precision <- conf_matrix$byClass['Pos Pred Value']
recall <- conf_matrix$byClass['Sensitivity'] # Recall is the same as Sensitivity
specificity <- conf_matrix$byClass['Specificity']
f1Score <- 2 * (precision * recall) / (precision + recall)
loss <- log_loss(test_probabilities, test_data$ObesityBinary)

# Printing the metrics
cat("Accuracy:", accuracy, "\n",
"Precision:", precision, "\n",
"Recall (Sensitivity):", recall, "\n",
"Specificity:", specificity, "\n",
"F1 Score:", f1Score, "\n",
"Log loss:", loss, "\n"
)
```

### Accuracy
* Accuracy represents the proportion of correctly classified instances out of the total instances.
* In this case, an accuracy of **0.6875** means that approximately **68.75%** of the instances were classified correctly by the model.

### Precision
* Precision represents the proportion of correctly predicted positive instances (true positives) out of all instances predicted as positive (true positives + false positives).
* A precision of **0.8039216** indicates that around **80.39%** of the instances predicted as positive are actually positive.

### Recall (Sensitivity)
* Recall, also known as sensitivity, represents the proportion of correctly predicted positive instances (true positives) out of all actual positive instances (true positives + false negatives).
* A recall of **0.5515695** suggests that the model correctly identifies approximately **55.16%** of the actual positive instances.

### Specificity
* Specificity represents the proportion of correctly predicted negative instances (true negatives) out of all actual negative instances (true negatives + false positives).
* A specificity of **0.8445596** means that approximately **84.46%** of the actual negative instances are correctly identified as negative by the model.

### F1 Score
* The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall, considering both false positives and false negatives.
* An F1 score of **0.6542553** suggests that the model achieves a balance between precision and recall, with a higher value indicating better performance.


### Log Loss
* Log loss measures the performance of a classification model where the prediction output is a probability value between 0 and 1.
* A log loss of **0.5553242** indicates the average negative log-likelihood of the predicted probabilities compared to the actual outcomes. Lower values suggest better performance.

## Conclusion

The analysis supports the alternative hypothesis (H1) that there is a significant relationship between the risk of obesity and the variables analyzed, including the frequency of consuming high-calorie foods, the number of main meals, consumption of food between meals, alcohol consumption, age, and gender. These factors are significant predictors of an individual's risk of obesity. Future research could explore additional variables or apply more complex models to enhance predictive accuracy and reduce log loss, further refining our understanding of obesity risk factors.
