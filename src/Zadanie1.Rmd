---
title: "Semestral Project: Analysis of Obesity Levels (Regression & Classification)"
author: "David Wagner"
date: "`r format(Sys.Date(), '%d.%m.%Y')`"
output: 
  html_document:
    toc: true
    toc_float: true
---

# Introduction

In this project, we delve into a dataset that captures the obesity levels among individuals from Mexico, Peru, and Colombia. The dataset can be found at [Kaggle](https://www.kaggle.com/datasets/aravindpcoder/obesity-or-cvd-risk-classifyregressorcluster?resource=download).
This rich dataset encompasses 2,111 records and 17 attributes, highlighting diverse eating habits, physical conditions, and demographic details of participants aged between 14 and 61. Our analysis aims to uncover patterns and predictions related to obesity levels through two main hypotheses:

* **Regression Hypothesis:** We hypothesize that an individual's lifestyle and physical condition—reflected through their eating habits, physical activity, and technology usage—are significant predictors of their age.

* **Classification Hypothesis:** We aim to classify individuals into distinct obesity levels (ranging from Underweight to Obesity III) by analyzing their eating habits, physical condition, and demographic information.

These hypotheses guide our exploration and analysis, contributing to a deeper understanding of the factors influencing obesity.


# Data Loading and Preprocessing

To initiate our analysis, the first step involves loading the dataset into our R environment. This dataset, stored as "ObesityDataSet.csv", encapsulates vital information on obesity levels among individuals from specific Latin American countries. Our analysis begins by setting the working directory to where the dataset is located and subsequently loading the dataset into R for inspection and preprocessing.

```{r load-data, message=FALSE, warning=FALSE}
library(readr)
library(tidyverse)
library(lobstr)
library(dplyr)
library(ggplot2)
library(magrittr)
library(data.table) # Implements the %like% operator
library(caret)
library(MASS)
# library(cutpointr)
library(purrr)
library(GGally)
library(ggplot2)
library(gplots)
library(pROC)


# Setting the working directory to where the dataset is located
# Uncomment the appropriate line below to match your directory structure

# setwd("C:/Users/dswag/Desktop/Zadania/OZNAL/Data")
setwd("C:/work/oznal/OZNAL_zadania/Data")

# Listing all files in the current working directory to verify the presence of our dataset
list.files()

# Reading the dataset into R
raw_data <- read_csv("ObesityDataSet.csv", col_names = TRUE, num_threads = 4)

# Displaying the first few rows of the dataset to ensure it's loaded correctly
head(raw_data)


# Renaming columns in the dataset to make them more descriptive
names(raw_data)[names(raw_data) == "FAVC"] <- "FreqConsHighCalFood"
names(raw_data)[names(raw_data) == "FCVC"] <- "FreqConsVegs"
names(raw_data)[names(raw_data) == "NCP"] <- "NumMainMeals"
names(raw_data)[names(raw_data) == "CAEC"] <- "ConsFoodBetwMeals"
names(raw_data)[names(raw_data) == "CH2O"] <- "ConsWaterDaily"
names(raw_data)[names(raw_data) == "CALC"] <- "ConsAlc"
names(raw_data)[names(raw_data) == "SCC"] <- "CalsConsMon"
names(raw_data)[names(raw_data) == "FAF"] <- "PhysActFreq"
names(raw_data)[names(raw_data) == "TUE"] <- "TimeTechDev"
names(raw_data)[names(raw_data) == "MTRANS"] <- "Trans"

# Identifying missing values in dataset
sum(is.na(raw_data))

# Identifying missing values per column
colSums(is.na(raw_data))

cleaned_data <- raw_data

# Optionally, if removing outliers introduced NAs, remove those rows
cleaned_data %<>% drop_na()
raw_data
cleaned_data

# Check for missing values again
sum(is.na(cleaned_data))

# Check data types of each column
str(cleaned_data)

# Convert all character columns to factors to represent categorical data
cleaned_data <- cleaned_data %>%
  mutate(across(where(is.character), as.factor))

# Check factorization of each column
str(cleaned_data)

# Example of standardization
# cleaned_data <- cleaned_data %>% mutate(across(where(is.numeric), scale))

# Removing duplicate rows from the datase, keeping only unique rows
cleaned_data <- cleaned_data %>% distinct()

cleaned_data

```

# Exploratory Data Analysis

Present an exploratory analysis of your dataset. This may include visualizations like histograms, box plots, or scatter plots to understand the distributions and relationships within your data.

```{r eda-data, message=FALSE, warning=FALSE}
# Select only the numeric columns for the pairs plot
numerical_data <- cleaned_data[sapply(cleaned_data, is.numeric) ]

# Use ggpairs to create the pairs plot, displaying both scatter plots and correlations
pairs(numerical_data)

# Calculate correlation matrix for numerical columns
cor_matrix <- cor(numerical_data, use = "complete.obs")

cor_matrix
```

**Positive Correlations:**

* Height and Weight: 0.457
* Weight and FreqConsVegs: 0.217
* Height and ConsWaterDaily: 0.220
* Weight and ConsWaterDaily: 0.204
* Height and PhysActFreq: 0.294
* FreqConsVegs and ConsWaterDaily: 0.081
* NumMainMeals and ConsWaterDaily: 0.075
* NumMainMeals and PhysActFreq: 0.128
* ConsWaterDaily and PhysActFreq: 0.165

**Negative Correlations:**

* Age and PhysActFreq: -0.148
* Age and TimeTechDev: -0.303

Most other correlations are weak (absolute value < 0.1), indicating little linear relationship.

```{r eda-data1, message=FALSE, warning=FALSE}
# Plotting the heatmap using base R's heatmap() function
heatmap.2(x = cor_matrix,
          scale = "none", 
          Colv = NA, 
          Rowv = NA, 
          dendrogram = "none",
          trace = "none", 
          col = colorRampPalette(c("blue", "white", "red"))(n = 299), 
          margin = c(5, 5), 
          main = "Correlation Matrix Heatmap",
          key = TRUE, # This ensures the color key (legend) is displayed
          keysize = 1.5)

```



# Regression Analysis: Predicting Age

## Hypothesis

State your regression hypothesis here, describing the expected relationship between lifestyle, physical condition variables, and age.

> We hypothesize that an individual’s lifestyle and physical condition—reflected through their eating habits, physical activity, and technology usage—are significant predictors of their age.

## Data Preparation

Detail the process of preparing your data for the regression model, including any feature selection or transformation.
```{r lm-data, message=FALSE, warning=FALSE}
# Create a reggresion formula
regression_formula <- Weight ~ Height + FreqConsHighCalFood + FreqConsVegs + NumMainMeals + PhysActFreq
```


## Model Building

Describe how you build your regression model, including the choice of model and any parameters you set.
```{r lm-data1, message=FALSE, warning=FALSE}
# Fit the linear regression model
model <- lm(regression_formula, data = cleaned_data)

# Summarize the model to view coefficients and other statistics
summary(model)

# If you want to check the diagnostic plots for assumptions
par(mfrow = c(2, 2))
plot(model)
```
### Intercept
Estimated weight when all other predictors are zero. In this case, it's -183.9069. However, given the nature of weight and the predictors in this context, interpretation of the intercept might not be meaningful

### Height
For each unit increase in height, weight is estimated to **increase** by approximately **138.3078** units

### FreqConsHighCalFoodyes
If an individual consumes high-calorie foods frequently (represented as "yes"), their weight is estimated to **increase** by about **13.9733** units compared to those who don't consume high-calorie foods frequently

### FreqConsVegs
For each unit increase in the frequency of consuming vegetables, weight is estimated to **increase** by about **12.0009** units

### NumMainMeals
It seems to have a **insignificant** effect of the number of main meals on weight, as the coefficient estimate is close to zero and the **p-value** is **greater** than the typical significance level of **0.05**. This suggests that the number of main meals doesn't significantly contribute to explaining the variation in weight in this model

### PhysActFreq
For each unit increase in the frequency of physical activity, weight is estimated to **decrease** by about **5.7346** units.

### Residuals
These are the differences between the observed and predicted values of the dependent variable (weight). The summary statistics for the residuals shows the spread of these differences:

* Min (-58.403) 
* 1Q (-14.501)
* Median (2.009)
* 3Q (15.874)
* Max (60.126)

### Residual standard error
This is an estimate of the standard deviation of the errors in the model. It indicates the average amount that the observed values deviate from the predicted values

### Multiple R-squared and Adjusted R-squared
These are measures of how well the independent variables explain the variation in the dependent variable. In this model, It suggests that about **33.38%** of the variability in weight is explained by the independent variables

### F-statistic and p-value
This tests the overall significance of the model. In this case, the F-statistic is **210.1** with a very low p-value, suggesting that the model as a whole is significant

```{r lm-data2, message=FALSE, warning=FALSE}
# Creating a new binary variable 'ObesityBinary' where 1 indicates Obesity (I, II, III) and 0 indicates otherwise
cleaned_data$ObesityBinary <- ifelse(cleaned_data$Nobesity %in% c("Obesity_Type_I", "Obesity_Type_II", "Obesity_Type_III"), 1, 0)

set.seed(123) # for reproducibility

# Calculate the size of the training set (80% of the dataset)
training_size <- floor(0.8 * nrow(cleaned_data))

# Sample indices for the training data
training_indices <- sample(seq_len(nrow(cleaned_data)), size = training_size)

# Split the data
train_data <- cleaned_data[training_indices, ]
test_data <- cleaned_data[-training_indices, ]

# Fit the model on the training data
model_train <- lm(regression_formula, data = train_data)

# Use the model to make predictions on the test data
predictions <- predict(model_train, newdata = test_data)
```

## Model Evaluation

Explain how you evaluate the performance of your regression model, including metrics like RMSE or MAE.
```{r lm-data3, message=FALSE, warning=FALSE}
# Calculate the Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE)
actuals <- test_data$Weight
mae <- mean(abs(actuals - predictions))
rmse <- sqrt(mean((predictions - actuals)^2))

# Print the RMSE and MAE
print(paste("RMSE on test data:", rmse))
print(paste("MAE on test data:", mae))

# Summarize the model to view coefficients and other statistics
summary(model_train)

# If you want to check the diagnostic plots for assumptions
par(mfrow = c(2, 2))
plot(model_train)

```


### RMSE (Root Mean Squared Error)
* RMSE measures the average magnitude of the errors between predicted and actual values, with higher weights given to larger errors due to squaring
* A lower RMSE value indicates better model performance, as it means the model's predictions are closer to the actual values on average.
* In out case, an RMSE of approximately **0.774** suggests that, on average, out model's predictions are off by around **0.774** units of the response variable (in this case, weight).

### MAE (Mean Absolute Error)
* MAE measures the average magnitude of the absolute errors between predicted and actual values.
* Like RMSE, a lower MAE value indicates better model performance, as it means the model's predictions are closer to the actual values on average.
* In our case, an MAE of approximately **0.637** suggests that, on average, our model's predictions are off by around **0.637** units of the response variable (in this case, weight).

Lower RMSE and MAE values indicate better performance, suggesting that our model is making more accurate predictions on the test data.


# Classification Analysis: NObesity Level Classification

## Hypothesis

State your classification hypothesis here, outlining how you expect the variables to predict the obesity level.

> We aim to classify individuals into distinct obesity levels (ranging from Underweight to Obesity III) by analyzing their eating habits, physical condition, and demographic information.

## Data Preparation

All necessary attributes have already been modified.

## Model Building

```{r glm-data, message=FALSE, warning=FALSE}
# Fit the logistic regression model on the training data
classification_model_train <- glm(ObesityBinary ~ FreqConsHighCalFood + NumMainMeals + ConsFoodBetwMeals + ConsAlc + Age + Gender, data = train_data, family = binomial())

# Summarize the model to view coefficients and other statistics
summary(classification_model_train)
```

### FreqConsHighCalFoodyes
Those who frequently consume high-calorie foods are **2.34** times more likely to be classified as obese.

### NumMainMeals
Each additional main meal increases the odds of obesity by approximately **1.11.**

### ConsFoodBetwMealsFrequently
Compared to those who **never** eat between meals, **frequent** eaters between meals have significantly **lower** odds of obesity.

### Age
Each year of age increases the odds of obesity by approximately **1.07**.

### GenderMale
Being **male** is associated with **lower** odds of obesity compared to being female.

```{r glm-data1, message=FALSE, warning=FALSE}
# Predict probabilities on the test data
test_probabilities <- predict(classification_model_train, newdata = test_data, type = "response")

# Calculate ROC curve and AUC on test data
roc_test_result <- roc(test_data$ObesityBinary, test_probabilities)
plot(roc_test_result)
auc_test <- auc(roc_test_result)

# Print AUC
print(paste("AUC on test data:", auc_test))
```

### AUC (Area Under the ROC Curve)
An AUC of **0.7777** indicates that the model performs reasonably well in distinguishing between the classes, with a higher value suggesting better discrimination than random chance.

```{r glm-data2, message=FALSE, warning=FALSE}
# Convert probabilities to binary outcomes based on a threshold
predicted_outcomes <- ifelse(test_probabilities > 0.5, 1, 0)

# Confusion Matrix
table(Predicted = predicted_outcomes, Actual = test_data$ObesityBinary)

# Creating the confusion matrix with caret
conf_matrix <- confusionMatrix(as.factor(predicted_outcomes), as.factor(test_data$ObesityBinary))
```

* True Negatives (TN): 117
* False Positives (FP): 30
* False Negatives (FN): 98
* True Positives (TP): 173

## Model Evaluation

Explain how you evaluate the performance of your classification model, using metrics like accuracy, precision, recall, or F1 score.

```{r glm-data3, message=FALSE, warning=FALSE}
# Plotting the confusion matrix with gplots
graphics.off()
heatmap.2(as.matrix(conf_matrix$table),
          Rowv = NA, Colv = NA,
          col = colorRampPalette(c("blue", "white", "red"))(n = 100),
          dendrogram = "none", trace = "none",
          key = TRUE, keysize = 1,
          main = "Confusion Matrix Heatmap",
          xlab = "Actual", ylab = "Predicted")

# Printing the overall statistics from the confusion matrix
print(conf_matrix$overall)

# Calculate log loss
log_loss <- function(predicted_probs, actual_outcomes) {
  N <- length(actual_outcomes)
  loss <- 0
  for (i in 1:N) {
    predicted_prob <- predicted_probs[i]
    actual_outcome <- actual_outcomes[i]
    instance_loss <- ifelse(actual_outcome == 1, -log(predicted_prob), -log(1 - predicted_prob))
    loss <- loss + instance_loss
  }
  loss <- loss / N
  return(loss)
}

# Specific metrics can be accessed directly
accuracy <- conf_matrix$overall['Accuracy']
precision <- conf_matrix$byClass['Pos Pred Value']
recall <- conf_matrix$byClass['Sensitivity'] # Recall is the same as Sensitivity
specificity <- conf_matrix$byClass['Specificity']
f1Score <- 2 * (precision * recall) / (precision + recall)
loss <- log_loss(test_probabilities, test_data$ObesityBinary)

# Printing the metrics
cat("Accuracy:", accuracy, "\n",
"Precision:", precision, "\n",
"Recall (Sensitivity):", recall, "\n",
"Specificity:", specificity, "\n",
"F1 Score:", f1Score, "\n",
"Log loss:", loss, "\n"
)
```

### Accuracy
* Accuracy represents the proportion of correctly classified instances out of the total instances.
* In this case, an accuracy of **0.6937799** means that approximately **69.38%** of the instances were classified correctly by the model.

### Precision
* Precision represents the proportion of correctly predicted positive instances (true positives) out of all instances predicted as positive (true positives + false positives).
* A precision of **0.7959184** indicates that around **79.59%** of the instances predicted as positive are actually positive.

### Recall (Sensitivity)
* Recall, also known as sensitivity, represents the proportion of correctly predicted positive instances (true positives) out of all actual positive instances (true positives + false negatives).
* A recall of **0.544186** suggests that the model correctly identifies approximately **54.42%** of the actual positive instances.

### Specificity
* Specificity represents the proportion of correctly predicted negative instances (true negatives) out of all actual negative instances (true negatives + false positives).
* A specificity of **0.8522167** means that approximately **85.22%** of the actual negative instances are correctly identified as negative by the model.

### F1 Score
* The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall, considering both false positives and false negatives.
* An F1 score of **0.6464088** suggests that the model achieves a balance between precision and recall, with a higher value indicating better performance.


### Log Loss
* Log loss measures the performance of a classification model where the prediction output is a probability value between 0 and 1.
* A log loss of **0.5623211** indicates the average negative log-likelihood of the predicted probabilities compared to the actual outcomes. Lower values suggest better performance.

# Conclusion

Summarize the key findings of your analyses, reflect on any limitations of your study, and suggest possible directions for future research.
